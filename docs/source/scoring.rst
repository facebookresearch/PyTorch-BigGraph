.. _scoring:

From entity embeddings to edge scores
=====================================

The goal of training is to embed each entity in :math:`\mathbb{R}^D` so that
the embeddings of two entities are a good proxy to predict whether there is
a relation of a certain type between them.

To be more precise, the goal is to learn an embedding for each entity and a
function for each relation type that takes two entity embeddings and assigns
them a score, with the goal of having positive relations achieve higher scores
than negative ones.

All the edges provided in the training set are considered positive instances.
In order to perform training, a set of negative edges is needed as well. These
are not provided by the user but instead generated by the system during training
(see :ref:`negative-sampling`), usually by fixing the left-hand side entity and
the relation type and sampling a new right-hand side entity, or vice versa. This
sampling scheme makes sense for large sparse graphs, where there is a low
probability that edges generated this way are true positives edges in the graph.

A priori, entity embeddings could take any value in :math:`\mathbb{R}^D`. Although,
in some cases (for example when restricting them to be within a certain ball, or
when comparing them using cosine distance), their "angle" will have greater
importance than their norm.

Per-relation scoring functions, however, must be expressible in a specific form
(the most common functions in the literature can be converted to such a representation).
In the current implementation, they are only allowed to transform the embedding
of one of the two sides, which is then compared to the un-transformed embedding
of the other side using a generic symmetric comparator function, which is the same
for all relations. Formally, for left- and right-hand side entities :math:`x`
and :math:`y` respectively, and for a relation type :math:`r`, the score is:

.. math::
    f_r(\theta_x, \theta_y) = c(\theta_x, g_r(\theta_y))

where :math:`\theta_x` and :math:`\theta_y` are the embeddings of :math:`x` and
:math:`y` respectively, :math:`f_r` is the scoring function for :math:`r`,
:math:`g_r` is the **operator** for :math:`r` and :math:`c` is the **comparator**.

Under "normal" circumstances (the so-called "standard" relations mode) the operator is solely applied to the right-hand
side entities. This is not the case when using :ref:`dynamic relations <dynamic-relations>`. Applying the operator to
both sides would oftentimes be redundant. Also, preferring one side over the other allows to break the symmetry and
capture the direction of the edge.

Embeddings
----------

Embeddings live in a :math:`D`-dimensional real space, where :math:`D` is
determined by the ``dimension`` configuration parameter.

Normally, each entity has its own embedding, which is entirely independent from
any other entity's embedding. When using :ref:`featurized entities <featurized-entities>`
however this works differently, and an entity's embedding will be the average of
the embeddings of its features.

If the ``max_norm`` configuration parameter is set, embeddings will be projected
onto the unit ball with radius ``max_norm`` after each parameter update.

To add a new type of embedding, one needs to subclass the :class:`torchbiggraph.model.AbstractEmbedding` class.

Global embeddings
-----------------

When the ``global_emb`` configuration option is active, each entity's embedding
will be translated by a vector that is specific to each entity type (and that is
learned at the same time as the embeddings).

.. _operators:

Operators
---------

The operators that are currently provided are:

* ``none``, no-op, which leaves the embeddings unchanged;
* ``translation``, which adds to the embedding a vector of the same dimension;
* ``diagonal``, multiplication of each dimension by a different coefficient
  (equivalent to multiplying by a diagonal matrix);
* ``linear``, linear map, i.e., multiplication by a full square matrix
* ``affine``, affine transformation, i.e., multiplication by a full square
  matrix plus addition of a vector.
* ``complex_diagonal``, which interprets the :math:`D`-dimensional real vector as a
  :math:`D/2`-dimensional complex vector (:math:`D` must be even) and then multiplies
  each entry by a different complex parameter.

All the operators' parameters are learned during training.

To define an additional operator, one must subclass the :class:`torchbiggraph.model.AbstractOperator` class
(or the :class:`torchbiggraph.model.AbstractDynamicOperator` one when using :ref:`dynamic relations <dynamic-relations>`)
and add an entry to the :class:`torchbiggraph.config.Operator` enum.

Comparators
-----------

The available comparators are:

* ``dot``, the dot-product, which computes the scalar or inner product of the two
  embedding vectors;
* ``cos``, the cos distance, which is the cosine of the angle between the two vectors
  or, equivalently, the dot product divided by the product of the vectors' norms.

Custom comparators need to extend the :class:`torchbiggraph.model.AbstractComparator` class
and add an item to the :class:`torchbiggraph.config.Comparator` enum.

Bias
----

If the ``bias`` configuration key is in use, then the first coordinate of the
embeddings will act as a bias in the comparator computation. This means that the
comparator will be computed on the last :math:`D - 1` entries of the vectors only,
and then both the first entries of the two vectors will be added to the result.

Coherent sets of configuration parameters
-----------------------------------------

While the parameters described in this chapter are exposed as uncoupled knobs
in the configuration file (to more closely match the implementation, and to allow
for more flexible tuning), some combinations of them are more sensible than others.

Apart from the default one, the following configuration has been found to work well:
``init_scale`` = 0.1, ``comparator`` = ``dot``, ``bias`` = true, ``loss_fn`` = ``logistic``, ``lr`` = 0.1.

Interpreting the scores
-----------------------

The scores will be tuned to have different meaning and become more suitable for
certain applications based on the :ref:`loss function <loss-calculation>` used during training.
Common options include ranking what other entities may be related to a given entity,
determining the probability that a certain relation exists between two given
entities, etc.

.. todo::
    Talk about what you can *do* with the trained embeddings (e.g., compute P(edge),
    k-nearest-neighbors, or training downstream classifiers on the features).
    Also, it would be nice to have a little script where one could manually feed
    it an edge and it could spit out a score. Or some nearest neighbor tool.

.. _dynamic-relations:

Dynamic relations
-----------------

.. caution:: This is an advanced topic!

Enabling the ``dynamic_relations`` flag in the configuration activates an alternative mode to be
used for graphs with a large number of relations (more than ~100 relations). In dynamic relation mode,
PBG runs with several modifications to its "standard" operation in order to support the large number of relations.
The differences are:

- The *number* of relations isn't provided in the config but is instead found in the input data, namely in the entity
  path, inside a :file:`dynamic_rel_count.txt` file. The settings of the relations, however, are still provided in the
  config file. This happens by providing a single relation config which will act as a "template" for all other ones, by
  being duplicated an appropriate number of times. One can think of this as the one relation in the config being
  "broadcasted" to the size of the relation list found in the :file:`dynamic_rel_count.txt` file.

- The batches of positive edges that are passed from the training loop into the model contain edges for multiple relation
  types at the same time (instead of each batch coming entirely from the same relation type). This introduces some performance challenges
  in how the operators are applied to the embeddings, as instead of a single operator with a single set of parameters
  applied to all edges, there might be a different one for each edge. The previous property ensures that all the operators
  are of the same type, so just their parameters might differ from one row to another. To account for this, the operators
  for dynamic relations are implemented differently, with a single operator object containing the parameters for all
  relation types. This implementation detail should be transparent as for how the operators are applied to the embeddings,
  but might come up when retrieving the parameters at the end of training.

- With non-dynamic relations, the operator is applied to the embedding of the right-hand side entity of the edge, whereas
  the embedding of the left-hand side entity is left unchanged. In a given batch, denote the :math:`i`-th positive edge
  by :math:`(x_i, r, y_i)` (:math:`x_i` and :math:`y_i` being the left- and right-hand side entities, :math:`r` being the
  relation type). For each of the positive edges, denote its :math:`j`-th negative sample :math:`(x_i, r, y'_{i,j})`.
  Due to :ref:`same-batch negative sampling <same-batch-negatives-sampling>` it may occur that the same right-hand side
  entity is used as a negative for several positives, that is, that :math:`y'_{i_1,j_1} = y'_{i_2,j_2}` for
  :math:`i_1 \neq i_2`. However, since it's the same relation type :math:`r` for all negatives, all the right-hand side
  entities will be transformed in the same way (i.e., passed through :math:`r`'s operator) no matter what positive edge
  they are a negative for. we need to apply the operator of :math:`r` to all of them, hence the total number of operator
  evaluations is equal to the number of positives and negatives.

  In case of dynamic relations the batch contains edges of the form :math:`(x_i, r_i, y_i)`, with possibly a different
  :math:`r_i` for each :math:`i`. If negative sampling and operator application worked the same, it might end up being
  necessary to transform each right-hand side entity multiple times in several ways, once for each different relation
  type of the edges the entity is a negative for. This would multiply the number of required operations by a significant
  factor and cause a sensible performance hit.

  To counter this, operators are applied differently in case of dynamic relations. They are applied to *either* the
  left- *or* the right-hand side (never both at the same time), and a different set of parameters is used in each of
  these two cases. On an input edge :math:`(x_i, r_i, y_i)` both ways of applying the operators are performed (separately).
  For the negatives of the form :math:`(x'_{i,j}, r_i, y_i)` (i.e., with the left-hand side entity changed), the operator
  is only applied to the right-hand side. Symmetrically, on :math:`(x_i, r_i, y'_{i,j})`, the operator is only applied to
  the left-hand side. This means that the operator is ever only applied to the entities of the original positive input
  edge, not on the entities of the negatives. Thus the number of operator evaluations is equal to the number of input
  edges in the batch.

  One could imagine it as if, for each edge of a certain relation type, a reversed edge were added to the graph, of a
  symmetric relation type. For each of these edges, the operator is only applied to the right-hand side, just like with
  standard relations. However, when sampling negatives, only the left-hand side entities are replaced, whereas the
  right-hand ones are kept unchanged.

  For more insight about this, look also at the "reciprocal predicates" described in [this paper](https://arxiv.org/pdf/1806.07297.pdf).
